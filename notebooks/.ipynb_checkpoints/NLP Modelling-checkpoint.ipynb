{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore deprecation warnings in sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_dir=os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from data.multilabel import multilabel_sample_dataframe, multilabel_train_test_split\n",
    "from features.SparseInteractions import SparseInteractions\n",
    "from models.metrics import multi_multi_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "First, we'll load the entire training data set available from DrivenData. In order to make this notebook run, you will need to:\n",
    "\n",
    "Sign up for an account on DrivenData\n",
    "Join the Box-plots for education competition\n",
    "Download the competition data to the data folder in this repository. Files should be named TrainingSet.csv and TestSet.csv.\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_data = os.path.join(os.pardir, 'data', 'TrainingData.csv')\n",
    "path_to_test_data = os.path.join(os.pardir,'data','TestData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/TrainingData.csv'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_training_data, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400277 entries, 134338 to 415831\n",
      "Data columns (total 25 columns):\n",
      "Function                  400277 non-null object\n",
      "Use                       400277 non-null object\n",
      "Sharing                   400277 non-null object\n",
      "Reporting                 400277 non-null object\n",
      "Student_Type              400277 non-null object\n",
      "Position_Type             400277 non-null object\n",
      "Object_Type               400277 non-null object\n",
      "Pre_K                     400277 non-null object\n",
      "Operating_Status          400277 non-null object\n",
      "Object_Description        375493 non-null object\n",
      "Text_2                    88217 non-null object\n",
      "SubFund_Description       306855 non-null object\n",
      "Job_Title_Description     292743 non-null object\n",
      "Text_3                    109152 non-null object\n",
      "Text_4                    53746 non-null object\n",
      "Sub_Object_Description    91603 non-null object\n",
      "Location_Description      162054 non-null object\n",
      "FTE                       126071 non-null float64\n",
      "Function_Description      342195 non-null object\n",
      "Facility_or_Department    53886 non-null object\n",
      "Position_Extra            264764 non-null object\n",
      "Total                     395722 non-null float64\n",
      "Program_Description       304660 non-null object\n",
      "Fund_Description          202877 non-null object\n",
      "Text_1                    292285 non-null object\n",
      "dtypes: float64(2), object(23)\n",
      "memory usage: 79.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400277, 25)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purpose and computational efficiency, we will sample down to 10,000 rows from 400,277rows so that it's easy to quick to run our analysis. We will also create dummy variables for our labels and split our sampel dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['Function',\n",
    "          'Use',\n",
    "          'Sharing',\n",
    "          'Reporting',\n",
    "          'Student_Type',\n",
    "          'Position_Type',\n",
    "          'Object_Type', \n",
    "          'Pre_K',\n",
    "          'Operating_Status']\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "SAMPLE_SIZE = 40000\n",
    "sampling = multilabel_sample_dataframe(df, \n",
    "                            pd.get_dummies(df[LABELS]),\n",
    "                            size = SAMPLE_SIZE,\n",
    "                            min_count=25,\n",
    "                            seed=43)\n",
    "\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(sampling[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2,\n",
    "                                                               min_count=3,\n",
    "                                                               seed=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessing toolsÂ¶\n",
    "We need tools to preprocess our text and numeric data. We'll create those tools here. The combine_text_columns function will take a DataFrame of text columns and return a single series where all of the text in the columns has been joined together.\n",
    "\n",
    "We'll then create FunctionTransformer objects that select our text and numeric data from the dataframe.\n",
    "\n",
    "Finally, we create a custom scoring method that uses the multi_multi_log_loss function that is the evaluation metric for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "\n",
    "def combine_text_columns(data_frame, to_drop = NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" Takes the dataset as read in, drops the non-feature, non-text columns and\n",
    "        then combines all of the text columns into a single vector that has all of\n",
    "        the text for a row.\n",
    "        \n",
    "        :param data_frame: The data as read in with read_csv (no preprocessing necessary)\n",
    "        :param to_drop (optional): Removes the numeric and label columns by default.\n",
    "    \"\"\"\n",
    "    # drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # replace NaN with Blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # joins all of the text items in a row (axis=1) with a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_text_columns(df, to_drop=NUMERIC_COLUMNS + LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38     OTHER PURCHASED SERVICES  SCHOOL-WIDE SCHOOL P...\n",
       "70     Extra Duty Pay/Overtime For Support Personnel ...\n",
       "198    Supplemental *  Operation and Maintenance of P...\n",
       "209    REPAIR AND MAINTENANCE SERVICES  PUPIL TRANSPO...\n",
       "614     GENERAL EDUCATION LOCAL EDUCATIONAL AIDE,70 H...\n",
       "dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_data.fit_transform(sampling.head(5))\n",
    "get_numeric_data.fit_transform(sampling.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "log_loss_scorere = make_scorer(multi_multi_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL PIPELINE\n",
    "Now we'll train the final pipeline from the course that takes text and numeric data, does the necessary preprocessing, and trains the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyu.kim@ibm.com/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'non_negative'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'non_negative'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set a reasonable number of features before adding interactions\n",
    "chi_k = 300\n",
    "\n",
    "#create the pipeline object\n",
    "pl = Pipeline([\n",
    "        (\"union\", FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('Selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])), \n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector',get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                    non_negative=True, norm=None, binary=False,\n",
    "                                                    ngram_range=(1,2))),\n",
    "                    ('dim_red',SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "            ]))\n",
    "\n",
    "        ])\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
